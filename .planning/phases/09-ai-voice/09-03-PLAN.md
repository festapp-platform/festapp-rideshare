---
phase: 09-ai-voice
plan: 03
type: execute
wave: 2
depends_on: ["09-01"]
files_modified:
  - apps/web/app/(app)/assistant/page.tsx
  - apps/web/app/(app)/assistant/components/chat-interface.tsx
  - apps/web/app/(app)/assistant/components/voice-input.tsx
  - apps/web/app/(app)/assistant/components/intent-confirmation.tsx
  - apps/web/app/(app)/assistant/actions.ts
  - apps/web/app/(app)/hooks/use-ai-assistant.ts
  - apps/web/app/(app)/app-nav.tsx
autonomous: true

must_haves:
  truths:
    - "User can open AI assistant page and type a natural language command"
    - "User can use voice input (microphone button) that transcribes speech to text and sends it"
    - "AI returns parsed intent and user sees a confirmation card before any mutation executes"
    - "After confirmation, the action executes and user sees success/error feedback"
    - "Assistant chat shows conversation history with user messages and AI responses"
    - "Voice input works on web via Web Speech API"
  artifacts:
    - path: "apps/web/app/(app)/assistant/page.tsx"
      provides: "AI assistant page route"
      contains: "ChatInterface"
    - path: "apps/web/app/(app)/assistant/components/chat-interface.tsx"
      provides: "Chat UI with message history, input, and send"
      contains: "useAiAssistant"
    - path: "apps/web/app/(app)/assistant/components/voice-input.tsx"
      provides: "Microphone button using Web Speech API"
      contains: "webkitSpeechRecognition"
    - path: "apps/web/app/(app)/assistant/components/intent-confirmation.tsx"
      provides: "Confirmation card showing parsed intent params before execution"
      contains: "onConfirm"
    - path: "apps/web/app/(app)/assistant/actions.ts"
      provides: "Server actions for AI Edge Function call and action execution"
      contains: "sendAiMessage"
    - path: "apps/web/app/(app)/hooks/use-ai-assistant.ts"
      provides: "React hook managing chat state, AI calls, confirmation flow"
      contains: "useAiAssistant"
  key_links:
    - from: "apps/web/app/(app)/hooks/use-ai-assistant.ts"
      to: "apps/web/app/(app)/assistant/actions.ts"
      via: "server action calls to AI Edge Function"
      pattern: "sendAiMessage"
    - from: "apps/web/app/(app)/assistant/actions.ts"
      to: "supabase/functions/ai-assistant"
      via: "Edge Function invocation"
      pattern: "functions/v1/ai-assistant"
    - from: "apps/web/app/(app)/assistant/components/voice-input.tsx"
      to: "apps/web/app/(app)/assistant/components/chat-interface.tsx"
      via: "onTranscript callback"
      pattern: "onTranscript"
---

<objective>
AI assistant chat UI with voice input and confirmation flow. Users interact via text or voice, see parsed intents for confirmation, and execute ride operations.

Purpose: This is the user-facing AI assistant. It combines the chat interface, voice-to-text input (Web Speech API), intent display with confirmation cards, and action execution. The chat maintains conversation history for multi-turn context.

Output: AI assistant page at /assistant with chat interface, voice input, and confirmation flow
</objective>

<execution_context>
@/Users/miakh/.claude/get-shit-done/workflows/execute-plan.md
@/Users/miakh/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-ai-voice/09-01-SUMMARY.md
@packages/shared/src/constants/ai.ts
@packages/shared/src/validation/ai.ts
@apps/web/app/(app)/app-nav.tsx
@apps/web/app/(app)/messages/components/chat-view.tsx
@apps/web/app/(app)/components/share-button.tsx
</context>

<tasks>

<task type="auto">
  <name>Task 1: AI assistant server actions and useAiAssistant hook</name>
  <files>
    apps/web/app/(app)/assistant/actions.ts
    apps/web/app/(app)/hooks/use-ai-assistant.ts
  </files>
  <action>
Create `apps/web/app/(app)/assistant/actions.ts` — Server actions (use "use server" directive):

1. `sendAiMessage(message: string, conversationHistory: Array<{role: 'user'|'assistant', content: string}>)`:
   - Get Supabase client from cookies (server-side pattern used throughout app)
   - Get user session, return error if not authenticated
   - Call Edge Function: `supabase.functions.invoke('ai-assistant', { body: { message, conversation_history: conversationHistory } })`
   - Return the parsed response (intent + reply) or error

2. `executeAiAction(action: string, params: Record<string, unknown>)`:
   - Get Supabase client from cookies
   - Get user session
   - Switch on action:
     - `create_ride`: Call compute-route Edge Function to get route_geometry, then insert ride via `supabase.from('rides').insert(...)`. Map AI params (origin_address, destination_address, departure_date + departure_time -> departure_time timestamp, available_seats, price_per_seat, notes) to ride schema.
     - `search_rides`: Call `supabase.rpc('nearby_rides', ...)` — need to geocode addresses first via compute-route or use simpler text match as fallback
     - `book_seat`: Call `supabase.rpc('book_seat', { p_ride_id: params.ride_id, p_seats: params.seats ?? 1 })`
     - `cancel_booking`: Call `supabase.rpc('cancel_booking', { p_booking_id: params.booking_id, p_reason: params.reason })`
     - `edit_ride`: Call `supabase.from('rides').update(...)` with provided fields
     - `complete_ride`: Call `supabase.rpc('complete_ride', { p_ride_id: params.ride_id })`
   - Return `{ success: boolean, data?: any, error?: string }`

Create `apps/web/app/(app)/hooks/use-ai-assistant.ts` — Client hook:

State: `messages` array (role + content + intent? + confirmed?), `isLoading`, `pendingConfirmation` (intent waiting for user confirm/reject)

Functions:
- `sendMessage(text: string)`: Add user message to history, call `sendAiMessage` server action, add assistant response. If intent with needs_confirmation=true, set `pendingConfirmation`.
- `confirmAction()`: Call `executeAiAction` with pending intent's action and params. Clear pendingConfirmation. Add result message.
- `rejectAction()`: Clear pendingConfirmation. Add "Action cancelled" message.
- `clearHistory()`: Reset messages array.

Conversation history: Pass last 10 messages (role + content only) to server action for context.
  </action>
  <verify>TypeScript compiles: `cd /Users/miakh/source/festapp-rideshare && pnpm turbo typecheck --filter=@festapp/web`</verify>
  <done>Server actions call AI Edge Function and execute ride operations. useAiAssistant hook manages chat state with confirmation flow for mutations.</done>
</task>

<task type="auto">
  <name>Task 2: Chat interface, voice input, intent confirmation components, and nav integration</name>
  <files>
    apps/web/app/(app)/assistant/page.tsx
    apps/web/app/(app)/assistant/components/chat-interface.tsx
    apps/web/app/(app)/assistant/components/voice-input.tsx
    apps/web/app/(app)/assistant/components/intent-confirmation.tsx
    apps/web/app/(app)/app-nav.tsx
  </files>
  <action>
Create `apps/web/app/(app)/assistant/components/voice-input.tsx` — Client component:
- Microphone button that toggles recording state
- Uses Web Speech API: `window.webkitSpeechRecognition || window.SpeechRecognition`
- Accept `lang` prop (default 'cs-CZ', support 'sk-SK', 'en-US') and set `recognition.lang = lang`
- Configure: `recognition.continuous = false`, `recognition.interimResults = false`
- The parent assistant page manages a language selector dropdown (CS/SK/EN) stored in state, passed to both VoiceInput (for speech recognition) and to the AI Edge Function request body (as `language` hint field) to keep them synchronized
- On result: extract transcript, call `onTranscript(text)` prop
- Visual states: idle (gray mic icon), recording (red pulsing mic icon with "Listening..." text), error (show toast via sonner)
- Handle not-supported browsers: hide button, show tooltip "Voice input not supported in this browser"
- Props: `onTranscript: (text: string) => void`, `disabled?: boolean`

Create `apps/web/app/(app)/assistant/components/intent-confirmation.tsx` — Client component:
- Card showing parsed intent for user confirmation
- Props: `intent: { action, params, display_text }`, `onConfirm: () => void`, `onReject: () => void`, `isExecuting: boolean`
- Display: action name as badge (e.g., "Create Ride", "Book Seat"), key params in a readable list (origin -> destination, date, seats, etc.), display_text from AI
- Two buttons: "Confirm" (primary, green) and "Cancel" (secondary, gray)
- Show spinner on Confirm while executing
- Format params nicely: dates as human-readable, addresses as-is, seats as "X seat(s)"

Create `apps/web/app/(app)/assistant/components/chat-interface.tsx` — Client component:
- Uses `useAiAssistant()` hook
- Chat layout: scrollable message list + fixed input bar at bottom
- Messages: user messages right-aligned (blue bubble), assistant messages left-aligned (gray bubble)
- When `pendingConfirmation` is set, show IntentConfirmation card inline in chat
- Input bar: text input + send button + VoiceInput button
- VoiceInput onTranscript: set input value and auto-submit (call sendMessage)
- Auto-scroll to bottom on new messages
- Empty state: friendly welcome message in Czech ("Ahoj! Jsem tvůj asistent pro spolujízdy. Napiš mi co potřebuješ, nebo použij mikrofon.") with example commands
- Loading state: typing indicator (three dots animation) while waiting for AI response

Create `apps/web/app/(app)/assistant/page.tsx` — Server component:
- Simple page wrapper with title "AI Assistant" / "AI Asistent"
- Render ChatInterface client component
- Standard page layout consistent with other (app) pages

Update `apps/web/app/(app)/app-nav.tsx`:
- Add "AI Assistant" nav item with a sparkles/bot icon (from lucide-react: `Bot` or `Sparkles`)
- Add to sidebar as secondary item (like Community and My Impact)
- Route: `/assistant`
  </action>
  <verify>Run `cd /Users/miakh/source/festapp-rideshare && pnpm turbo build --filter=@festapp/web` — builds without errors. Visit /assistant page in browser to see chat interface.</verify>
  <done>AI assistant page at /assistant with chat interface, voice input via Web Speech API, and intent confirmation cards. Nav item added to sidebar. Users can type or speak commands, see parsed intents, confirm/reject mutations.</done>
</task>

</tasks>

<verification>
1. Web app builds: `pnpm turbo build --filter=@festapp/web`
2. /assistant page renders chat interface with input bar and microphone button
3. Typing a message calls AI Edge Function and shows response
4. Voice input captures speech and populates input field
5. Mutation intents show confirmation card; search intents show results directly
6. Confirm executes action; Cancel dismisses
</verification>

<success_criteria>
- User can navigate to /assistant from sidebar
- User can type natural language commands and receive AI responses
- User can use microphone for voice input (Web Speech API)
- Mutation actions show confirmation card before execution
- Chat maintains conversation history for multi-turn context
- Voice input supports Czech, Slovak, and English speech recognition
</success_criteria>

<output>
After completion, create `.planning/phases/09-ai-voice/09-03-SUMMARY.md`
</output>
